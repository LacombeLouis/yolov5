{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Tutorial for conformalized quantile regression (CQR)\n",
        "\n",
        "We will use the sklearn california housing dataset as the base for the\n",
        "comparison of the different methods available on MAPIE. Two classes will\n",
        "be used: :class:`~mapie.quantile_regression.MapieQuantileRegressor` for CQR\n",
        "and :class:`~mapie.regression.MapieRegressor` for the other methods.\n",
        "\n",
        "For this example, the estimator will be :class:`~lightgbm.LGBMRegressor` with\n",
        "``objective=\"quantile\"`` as this is a necessary component for CQR, the\n",
        "regression needs to be from a quantile regressor.\n",
        "\n",
        "For the conformalized quantile regression (CQR), we will use a split-conformal\n",
        "method meaning that we will split the training set into a training and\n",
        "calibration set. This means using\n",
        ":class:`~mapie.quantile_regression.MapieQuantileRegressor` with ``cv=\"split\"``\n",
        "and the ``alpha`` parameter already defined. Recall that the ``alpha`` is\n",
        "`1 - target coverage`.\n",
        "\n",
        "For the other type of conformal methods, they are chosen with the\n",
        "parameter ``method`` of :class:`~mapie.regression.MapieRegressor` and the\n",
        "parameter ``cv`` is the strategy for cross-validation. In this method, to use a\n",
        "\"leave-one-out\" strategy, one would have to use ``cv=-1`` where a positive\n",
        "value would indicate the number of folds for a cross-validation strategy.\n",
        "Note that for the jackknife+ after boostrap, we need to use the\n",
        "class :class:`~mapie.subsample.Subsample` (note that the `alpha` parameter is\n",
        "defined in the ``predict`` for these methods).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import TextArea, AnnotationBbox\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split, KFold\n",
        "from sklearn.datasets import (\n",
        "    fetch_california_housing, load_diabetes, make_regression,\n",
        "    make_sparse_uncorrelated, make_friedman1, make_friedman2, make_friedman3\n",
        ")\n",
        "from scipy.stats import randint, uniform\n",
        "import seaborn as sns\n",
        "\n",
        "from mapie.metrics import (\n",
        "    regression_coverage_score,\n",
        "    regression_mean_width_score\n",
        "    )\n",
        "from mapie.regression import MapieRegressor\n",
        "from mapie.subsample import Subsample\n",
        "from mapie.quantile_regression import MapieQuantileRegressor\n",
        "\n",
        "\n",
        "random_state = 23\n",
        "rng = np.random.default_rng(random_state)\n",
        "round_to = 3\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../../')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.utils_numpy import get_binning_groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data\n",
        "The target variable of this dataset is the median house value for the\n",
        "California districts. This dataset is composed of 8 features, including\n",
        "variables such as the age of the house, the median income of the\n",
        "neighborhood, the average numbe rooms or bedrooms or even the location in\n",
        "latitude and longitude. In total there are around 20k observations.\n",
        "As the value is expressed in thousands of $ we will multiply it by 100 for\n",
        "better visualization (note that this will not affect the results).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data(name, n_samples=10000, n_features=15):\n",
        "    if name == \"california\":\n",
        "        data = fetch_california_housing(as_frame=True)\n",
        "        X = pd.DataFrame(data=data.data, columns=data.feature_names).drop(columns=['Latitude', 'Longitude'], axis=1)\n",
        "        y = pd.Series(data=data.target)\n",
        "    elif name == \"diabetes\":\n",
        "        data = load_diabetes(as_frame=True)\n",
        "        X = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "        y = pd.Series(data=data.target)\n",
        "        X[\"sex1\"] = 0\n",
        "        X[\"sex2\"] = 0\n",
        "        X.loc[(X[\"sex\"] == np.unique(X[\"sex\"])[0]), \"sex1\"] = 1\n",
        "        X.loc[(X[\"sex\"] == np.unique(X[\"sex\"])[1]), \"sex2\"] = 1\n",
        "        X = X.drop([\"sex\"], axis=1)\n",
        "    elif name == \"make_regression\":\n",
        "        data = make_regression(n_samples, n_features)\n",
        "        X = pd.DataFrame(data[0])\n",
        "        y = pd.Series(data[1])\n",
        "    elif name == \"sparse_uncorrelated\":\n",
        "        data = make_sparse_uncorrelated(n_samples, n_features)\n",
        "        X = pd.DataFrame(data[0])\n",
        "        y = pd.Series(data[1])\n",
        "    elif name == \"friedman1\":\n",
        "        data = make_friedman1(n_samples, n_features)\n",
        "        X = pd.DataFrame(data[0])\n",
        "        y = pd.Series(data[1])\n",
        "    elif name == \"friedman2\":\n",
        "        data = make_friedman2(n_samples)\n",
        "        X = pd.DataFrame(data[0])\n",
        "        y = pd.Series(data[1])\n",
        "    elif name == \"friedman3\":\n",
        "        data = make_friedman3(n_samples)\n",
        "        X = pd.DataFrame(data[0])\n",
        "        y = pd.Series(data[1])\n",
        "    elif name == \"heteroscedastic\":\n",
        "        X = np.linspace(0, 5, n_samples)\n",
        "        y = pd.Series((((3*X)+5) + (np.random.normal(0, 1, len(X)) * X)))\n",
        "        X = pd.DataFrame(X)\n",
        "    elif name == \"homoscedastic\":\n",
        "        X = np.linspace(0, 5, n_samples)\n",
        "        y = pd.Series((((3*X)+5) + (np.random.normal(0, 1, len(X)))))\n",
        "        X = pd.DataFrame(X)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"friedman3\"\n",
        "X, y = get_data(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir_ = \"latex_tables/\"+name+\"/\"\n",
        "if not os.path.exists(dir_):\n",
        "    os.mkdir(dir_)\n",
        "\n",
        "dir_ = \"images/\"+name+\"/\"\n",
        "if not os.path.exists(dir_):\n",
        "    os.mkdir(dir_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize the dataset by showing the correlations between the\n",
        "independent variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_ = pd.DataFrame(y).rename(columns={0: \"y\"})\n",
        "df = pd.concat([X, y_], axis=1)\n",
        "pear_corr = df.corr(method='pearson')\n",
        "pear_corr.style.background_gradient(cmap='Greens', axis=0)\n",
        "pear_corr.to_latex(\"latex_tables/\"+name+\"/\"+name+\"_corr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's visualize a histogram of the price of the houses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
        "axs.hist(y, bins=50)\n",
        "name_y = y_.columns[0]\n",
        "axs.set_xlabel(name_y)\n",
        "axs.set_title(\"Histogram of \"+name_y)\n",
        "# axs.xaxis.set_major_formatter(FormatStrFormatter('%.0f' + \"k\"))\n",
        "plt.savefig(\"images/\"+name+\"/\"+name+\"_histy.png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "X_ = pca.fit_transform(X)\n",
        "df_ = pd.concat([pd.DataFrame(X_).rename(columns={0: \"x\"}), pd.DataFrame(y).rename(columns={0: \"y\"})], axis=1)\n",
        "sns.histplot(df_, x=\"x\", y=name_y, bins=50)\n",
        "# plt.scatter(X_, y)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.savefig(\"images/\"+name+\"/\"+name+\"_scatterxy.png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now create the different splits for the dataset, with a training,\n",
        "calibration and test set. Recall that the calibration set is used for\n",
        "calibrating the prediction intervals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    random_state=random_state\n",
        ")\n",
        "X_train, X_calib, y_train, y_calib = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    random_state=random_state\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Optimizing estimator\n",
        "Before estimating uncertainties, let's start by optimizing the base model\n",
        "in order to reduce our prediction error. We will use the\n",
        ":class:`~lightgbm.LGBMRegressor` in the quantile setting. The optimization\n",
        "is performed using :class:`~sklearn.model_selection.RandomizedSearchCV`\n",
        "to find the optimal model to predict the house prices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = LGBMRegressor(\n",
        "    objective='quantile',\n",
        "    alpha=0.5,\n",
        "    random_state=random_state\n",
        ")\n",
        "params_distributions = dict(\n",
        "    num_leaves=randint(low=10, high=50),\n",
        "    max_depth=randint(low=3, high=20),\n",
        "    n_estimators=randint(low=50, high=300),\n",
        "    learning_rate=uniform()\n",
        ")\n",
        "optim_model = RandomizedSearchCV(\n",
        "    estimator,\n",
        "    param_distributions=params_distributions,\n",
        "    n_jobs=-1,\n",
        "    n_iter=100,\n",
        "    cv=KFold(n_splits=5, shuffle=True),\n",
        "    verbose=-1\n",
        ")\n",
        "optim_model.fit(X_train, y_train)\n",
        "estimator = optim_model.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Comparison of MAPIE methods\n",
        "We will now proceed to compare the different methods available in MAPIE used\n",
        "for uncertainty quantification on regression settings. For this tutorial we\n",
        "will compare the \"naive\", \"Jackknife plus after Bootstrap\", \"cv plus\" and\n",
        "\"conformalized quantile regression\". Please have a look at the theoretical\n",
        "description of the documentation for more details on these methods.\n",
        "\n",
        "We also create two functions, one to sort the dataset in increasing values\n",
        "of ``y_test`` and a plotting function, so that we can plot all predictions\n",
        "and prediction intervals for different conformal methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def sort_y_values(y_test, y_pred, y_pis):\n",
        "    \"\"\"\n",
        "    Sorting the dataset in order to make plots using the fill_between function.\n",
        "    \"\"\"\n",
        "    indices = np.argsort(y_test)\n",
        "    y_test_sorted = np.array(y_test)[indices]\n",
        "    y_pred_sorted = y_pred[indices]\n",
        "    y_lower_bound = y_pis[:, 0, 0][indices]\n",
        "    y_upper_bound = y_pis[:, 1, 0][indices]\n",
        "    return y_test_sorted, y_pred_sorted, y_lower_bound, y_upper_bound\n",
        "\n",
        "\n",
        "def plot_prediction_intervals(\n",
        "    title,\n",
        "    axs,\n",
        "    y_test_sorted,\n",
        "    y_pred_sorted,\n",
        "    lower_bound,\n",
        "    upper_bound,\n",
        "    coverage,\n",
        "    width,\n",
        "    num_plots_idx\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot of the prediction intervals for each different conformal\n",
        "    method.\n",
        "    \"\"\"\n",
        "    # axs.yaxis.set_major_formatter(FormatStrFormatter('%.0f' + \"k\"))\n",
        "    # axs.xaxis.set_major_formatter(FormatStrFormatter('%.0f' + \"k\"))\n",
        "\n",
        "    lower_bound_ = np.take(lower_bound, num_plots_idx)\n",
        "    y_pred_sorted_ = np.take(y_pred_sorted, num_plots_idx)\n",
        "    y_test_sorted_ = np.take(y_test_sorted, num_plots_idx)\n",
        "\n",
        "    error = y_pred_sorted_-lower_bound_\n",
        "\n",
        "    warning1 = y_test_sorted_ > y_pred_sorted_+error\n",
        "    warning2 = y_test_sorted_ < y_pred_sorted_-error\n",
        "    warnings = warning1 + warning2\n",
        "    axs.errorbar(\n",
        "        y_test_sorted_[~warnings],\n",
        "        y_pred_sorted_[~warnings],\n",
        "        yerr=error[~warnings],\n",
        "        capsize=5, marker=\"o\", elinewidth=2, linewidth=0,\n",
        "        label=\"Inside prediction interval\"\n",
        "        )\n",
        "    axs.errorbar(\n",
        "        y_test_sorted_[warnings],\n",
        "        y_pred_sorted_[warnings],\n",
        "        yerr=error[warnings],\n",
        "        capsize=5, marker=\"o\", elinewidth=2, linewidth=0, color=\"red\",\n",
        "        label=\"Outside prediction interval\"\n",
        "        )\n",
        "    axs.scatter(\n",
        "        y_test_sorted_[warnings],\n",
        "        y_test_sorted_[warnings],\n",
        "        marker=\"*\", color=\"green\",\n",
        "        label=\"True value\"\n",
        "    )\n",
        "    axs.set_xlabel(\"True house prices in $\")\n",
        "    axs.set_ylabel(\"Prediction of house prices in $\")\n",
        "    median_lim = np.median([axs.get_xlim()])\n",
        "    min_lim = np.min([axs.get_xlim(), axs.get_ylim()])\n",
        "    max_lim = np.max([axs.get_xlim(), axs.get_ylim()])\n",
        "    ab = AnnotationBbox(\n",
        "        TextArea(\n",
        "            f\"Coverage: {np.round(coverage, round_to)}\\n\"\n",
        "            + f\"Interval width: {np.round(width, round_to)}\"\n",
        "        ),\n",
        "        xy=(median_lim, max_lim),\n",
        "        )\n",
        "    lims = [\n",
        "        min_lim,  # min of both axes\n",
        "        max_lim,  # max of both axes\n",
        "    ]\n",
        "    axs.plot(lims, lims, '--', alpha=0.75, color=\"black\", label=\"x=y\")\n",
        "    axs.add_artist(ab)\n",
        "    axs.set_title(title, fontweight='bold')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We proceed to using MAPIE to return the predictions and prediction intervals.\n",
        "We will use an $\\alpha=0.2$, this means a target coverage of 0.8\n",
        "(recall that this parameter needs to be initialized directly when setting\n",
        ":class:`~mapie.quantile_regression.MapieQuantileRegressor` and when using\n",
        ":class:`~mapie.regression.MapieRegressor`, it needs to be set in the\n",
        "``predict``).\n",
        "Note that for the CQR, there are two options for ``cv``:\n",
        "\n",
        "* ``cv=\"split\"`` (by default), the split-conformal where MAPIE trains the\n",
        "  model on a training set and then calibrates on the calibration set.\n",
        "* ``cv=\"prefit\"`` meaning that you can train your models with the correct\n",
        "  quantile values (must be given in the following order:\n",
        "  $(\\alpha, 1-(\\alpha/2), 0.5)$ and given to MAPIE as an iterable\n",
        "  object. (Check the examples for how to use prefit in MAPIE)\n",
        "\n",
        "Additionally, note that there is a list of accepted models by\n",
        ":class:`~mapie.quantile_regression.MapieQuantileRegressor`\n",
        "(``quantile_estimator_params``) and that we will use symmetrical residuals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "STRATEGIES = {\n",
        "    \"naive\": {\"method\": \"naive\"},\n",
        "    \"cv_plus\": {\"method\": \"plus\", \"cv\": 10},\n",
        "    \"jackknife_plus_ab\": {\"method\": \"plus\", \"cv\": Subsample(n_resamplings=50)},\n",
        "    \"cqr\": {\"method\": \"quantile\", \"cv\": \"split\", \"alpha\": 0.2},\n",
        "}\n",
        "y_pred, y_pis = {}, {}\n",
        "y_test_sorted, y_pred_sorted, lower_bound, upper_bound = {}, {}, {}, {}\n",
        "coverage, width = {}, {}\n",
        "for strategy, params in STRATEGIES.items():\n",
        "    if strategy == \"cqr\":\n",
        "        mapie = MapieQuantileRegressor(estimator, **params)\n",
        "        mapie.fit(X_train, y_train, X_calib=X_calib, y_calib=y_calib)\n",
        "        y_pred[strategy], y_pis[strategy] = mapie.predict(X_test)\n",
        "    else:\n",
        "        mapie = MapieRegressor(estimator, **params)\n",
        "        mapie.fit(X_train, y_train)\n",
        "        y_pred[strategy], y_pis[strategy] = mapie.predict(X_test, alpha=0.2)\n",
        "    (\n",
        "        y_test_sorted[strategy],\n",
        "        y_pred_sorted[strategy],\n",
        "        lower_bound[strategy],\n",
        "        upper_bound[strategy]\n",
        "    ) = sort_y_values(y_test, y_pred[strategy], y_pis[strategy])\n",
        "    coverage[strategy] = regression_coverage_score(\n",
        "        y_test,\n",
        "        y_pis[strategy][:, 0, 0],\n",
        "        y_pis[strategy][:, 1, 0]\n",
        "        )\n",
        "    width[strategy] = regression_mean_width_score(\n",
        "        y_pis[strategy][:, 0, 0],\n",
        "        y_pis[strategy][:, 1, 0]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now proceed to the plotting stage, note that we only plot 2% of the\n",
        "observations in order to not crowd the plot too much.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perc_obs_plot = 0.02\n",
        "num_plots = rng.choice(\n",
        "    len(y_test), int(perc_obs_plot*len(y_test)), replace=False\n",
        "    )\n",
        "fig, axs = plt.subplots(2, 2, figsize=(15, 13))\n",
        "coords = [axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1]]\n",
        "for strategy, coord in zip(STRATEGIES.keys(), coords):\n",
        "    plot_prediction_intervals(\n",
        "        strategy,\n",
        "        coord,\n",
        "        y_test_sorted[strategy],\n",
        "        y_pred_sorted[strategy],\n",
        "        lower_bound[strategy],\n",
        "        upper_bound[strategy],\n",
        "        coverage[strategy],\n",
        "        width[strategy],\n",
        "        num_plots\n",
        "        )\n",
        "lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]\n",
        "lines, labels = [sum(_, []) for _ in zip(*lines_labels)]\n",
        "plt.legend(\n",
        "    lines[:4], labels[:4],\n",
        "    loc='upper center',\n",
        "    bbox_to_anchor=(0, -0.15),\n",
        "    fancybox=True,\n",
        "    shadow=True,\n",
        "    ncol=2\n",
        ")\n",
        "plt.savefig(\"images/\"+name+\"/\"+name+\"_plotintervals.png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We notice more adaptability of the prediction intervals for the\n",
        "conformalized quantile regression while the other methods have fixed\n",
        "interval width. Indeed, as the prices get larger, the prediction intervals\n",
        "are increased with the increase in price.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_coverages_widths_by_bins(\n",
        "    want,\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    lower_bound,\n",
        "    upper_bound,\n",
        "    STRATEGIES,\n",
        "    bins,\n",
        "    bin_strategy\n",
        "):\n",
        "    \"\"\"\n",
        "    Given the results from MAPIE, this function split the data\n",
        "    according the the test values into bins and calculates coverage\n",
        "    or width per bin.\n",
        "    \"\"\"\n",
        "    # cuts = []\n",
        "    # cuts_ = pd.qcut(y_test[\"naive\"], bins).unique()[:-1]\n",
        "    # for item in cuts_:\n",
        "    #     cuts.append(item.left)\n",
        "    # cuts.append(cuts_[-1].right)\n",
        "    # cuts.append(np.max(y_test[\"naive\"])+1)\n",
        "    bins, _ = get_binning_groups(y_test[\"naive\"], bins+1, bin_strategy)\n",
        "    recap = {}\n",
        "    for i in range(len(bins) - 1):\n",
        "        cut1, cut2 = bins[i], bins[i+1]\n",
        "        name = f\"[{np.round(cut1, 2)}, {np.round(cut2, 2)}]\"\n",
        "        recap[name] = []\n",
        "        for strategy in STRATEGIES:\n",
        "            indices = np.where(\n",
        "                (y_test[strategy] > cut1) * (y_test[strategy] <= cut2)\n",
        "                )\n",
        "            y_test_trunc = np.take(y_test[strategy], indices)\n",
        "            y_low_ = np.take(lower_bound[strategy], indices)\n",
        "            y_high_ = np.take(upper_bound[strategy], indices)\n",
        "            if want == \"coverage\":\n",
        "                recap[name].append(regression_coverage_score(\n",
        "                    y_test_trunc[0],\n",
        "                    y_low_[0],\n",
        "                    y_high_[0]\n",
        "                ))\n",
        "            elif want == \"width\":\n",
        "                recap[name].append(\n",
        "                    regression_mean_width_score(y_low_[0], y_high_[0])\n",
        "                )\n",
        "    recap_df = pd.DataFrame(recap, index=STRATEGIES)\n",
        "    return recap_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To confirm these insights, we will now observe what happens when we plot\n",
        "the conditional coverage and interval width on these intervals splitted by\n",
        "quantiles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bins = 10\n",
        "bin_strategy = \"quantile\"\n",
        "binned_data = get_coverages_widths_by_bins(\n",
        "    \"coverage\",\n",
        "    y_test_sorted,\n",
        "    y_pred_sorted,\n",
        "    lower_bound,\n",
        "    upper_bound,\n",
        "    STRATEGIES,\n",
        "    bins=bins,\n",
        "    bin_strategy=bin_strategy\n",
        ")\n",
        "\n",
        "binned_data.T.plot.bar(figsize=(12, 4))\n",
        "plt.axhline(0.80, ls=\"--\", color=\"k\")\n",
        "plt.ylabel(\"Conditional coverage\")\n",
        "plt.xlabel(\"Binned house prices\")\n",
        "plt.xticks(rotation=345)\n",
        "plt.ylim(0.3, 1.0)\n",
        "plt.legend(loc=[1, 0])\n",
        "plt.savefig(\"images/\"+name+\"/\"+name+\"_barcoverage.png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What we observe from these results is that none of the methods seems to\n",
        "have conditional coverage at the target $1 - \\alpha$. However, we can\n",
        "clearly notice that the CQR seems to better adapt to large prices. Its\n",
        "conditional coverage is closer to the target coverage not only for higher\n",
        "prices, but also for lower prices where the other methods have a higher\n",
        "coverage than needed. This will very likely have an impact on the widths\n",
        "of the intervals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "binned_data = get_coverages_widths_by_bins(\n",
        "    \"width\",\n",
        "    y_test_sorted,\n",
        "    y_pred_sorted,\n",
        "    lower_bound,\n",
        "    upper_bound,\n",
        "    STRATEGIES,\n",
        "    bins=bins,\n",
        "    bin_strategy=bin_strategy\n",
        ")\n",
        "\n",
        "binned_data.T.plot.bar(figsize=(12, 4))\n",
        "plt.ylabel(\"Interval width\")\n",
        "plt.xlabel(\"Binned house prices\")\n",
        "plt.xticks(rotation=350)\n",
        "plt.legend(loc=[1, 0])\n",
        "plt.savefig(\"images/\"+name+\"/\"+name+\"_barintervals.png\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When observing the values of the the interval width we again see what was\n",
        "observed in the previous graphs with the interval widths. We can again see\n",
        "that the prediction intervals are larger as the price of the houses\n",
        "increases, interestingly, it's important to note that the prediction\n",
        "intervals are shorter when the estimator is more certain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 ('mapie_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "193ddd0459bf56ec2df4e0f45f021c5529ceee3254bb785350372ea00e0e07f5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
